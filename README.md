<!--
**peter-fields/peter-fields** is a âœ¨ _special_ âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- ðŸ”­ Iâ€™m currently working on ...
- ðŸŒ± Iâ€™m currently learning ...
- ðŸ‘¯ Iâ€™m looking to collaborate on ...
- ðŸ¤” Iâ€™m looking for help with ...
- ðŸ’¬ Ask me about ...
- ðŸ“« How to reach me: ...
- ðŸ˜„ Pronouns: ...
- âš¡ Fun fact: ...
-->

# Peter Fields

Website: https://peter-fields.github.io  
Google Scholar: https://scholar.google.com/citations?user=vWoUWkkAAAAJ

## Writing

- **Why Softmax? A Hypothesis Testing Perspective on Attention Weights**  
  https://peter-fields.github.io/why-softmax/  
  Softmax is ubiquitous in transformers, yet its role in attention can feel more heuristic than inevitable (at least to me). In this post, I try to make it feel more natural and show how this interpretation suggests useful diagnostics for the often circuit-like behavior of attention heads.

## Research interests

- Statistical physics perspectives on learning and generalization
- Mechanistic / mathematical views of representation learning (KL geometry, exponential families, energy-based models)
- Applications of stat phys and machine learning to biology

## Selected work

- OpenReview: *Understanding Energy-Based Modeling of Proteins via an Empirically Motivated Minimal Ground Truth Model*  
  https://openreview.net/pdf?id=vxn5QGPFyi

- arXiv: *Temperature-tuning trained energy functions improves generative performance*  
  https://www.arxiv.org/pdf/2512.09152

## Links

- GitHub: https://github.com/peter-fields
- LinkedIn: https://www.linkedin.com/in/peter-fields-8a9473106/
